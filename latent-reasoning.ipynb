{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":230366006,"sourceType":"kernelVersion"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install math-verify[antlr4_13_2]\n!pip install antlr4-python3-runtime==4.13.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:12:41.183714Z","iopub.execute_input":"2025-04-02T09:12:41.184024Z","iopub.status.idle":"2025-04-02T09:12:51.043137Z","shell.execute_reply.started":"2025-04-02T09:12:41.184001Z","shell.execute_reply":"2025-04-02T09:12:51.041892Z"}},"outputs":[{"name":"stdout","text":"Collecting math-verify[antlr4_13_2]\n  Downloading math_verify-0.7.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting latex2sympy2_extended==1.10.1 (from math-verify[antlr4_13_2])\n  Downloading latex2sympy2_extended-1.10.1-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from latex2sympy2_extended==1.10.1->math-verify[antlr4_13_2]) (1.13.1)\nRequirement already satisfied: antlr4-python3-runtime<=4.13.2,>=4.9.3 in /usr/local/lib/python3.10/dist-packages (from latex2sympy2_extended==1.10.1->math-verify[antlr4_13_2]) (4.9.3)\nCollecting antlr4-python3-runtime<=4.13.2,>=4.9.3 (from latex2sympy2_extended==1.10.1->math-verify[antlr4_13_2])\n  Downloading antlr4_python3_runtime-4.13.2-py3-none-any.whl.metadata (304 bytes)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->latex2sympy2_extended==1.10.1->math-verify[antlr4_13_2]) (1.3.0)\nDownloading latex2sympy2_extended-1.10.1-py3-none-any.whl (207 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading antlr4_python3_runtime-4.13.2-py3-none-any.whl (144 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading math_verify-0.7.0-py3-none-any.whl (28 kB)\nInstalling collected packages: antlr4-python3-runtime, latex2sympy2_extended, math-verify\n  Attempting uninstall: antlr4-python3-runtime\n    Found existing installation: antlr4-python3-runtime 4.9.3\n    Uninstalling antlr4-python3-runtime-4.9.3:\n      Successfully uninstalled antlr4-python3-runtime-4.9.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nomegaconf 2.3.0 requires antlr4-python3-runtime==4.9.*, but you have antlr4-python3-runtime 4.13.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed antlr4-python3-runtime-4.13.2 latex2sympy2_extended-1.10.1 math-verify-0.7.0\nRequirement already satisfied: antlr4-python3-runtime==4.13.2 in /usr/local/lib/python3.10/dist-packages (4.13.2)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW, Adam\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport datasets\nfrom peft import get_peft_model, LoraConfig\n\nimport gc\nimport re\nimport threading\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n\nimport polars as pl\nimport matplotlib.pyplot as plt\n\nfrom math_verify import parse, verify","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:12:51.044820Z","iopub.execute_input":"2025-04-02T09:12:51.045195Z","iopub.status.idle":"2025-04-02T09:13:17.770008Z","shell.execute_reply.started":"2025-04-02T09:12:51.045160Z","shell.execute_reply":"2025-04-02T09:13:17.769337Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"data_raw = pl.scan_parquet('hf://datasets/open-r1/OpenR1-Math-220k/data/train-*.parquet') # lazy load\nmodel_name = 'deepseek-ai/Deepseek-R1-Distill-Qwen-1.5B'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype=torch.float16, attn_implementation='sdpa')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:13:17.771188Z","iopub.execute_input":"2025-04-02T09:13:17.771777Z","iopub.status.idle":"2025-04-02T09:13:38.995049Z","shell.execute_reply.started":"2025-04-02T09:13:17.771753Z","shell.execute_reply":"2025-04-02T09:13:38.994183Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3317bd7831747c8b3536a1975f05f1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"175d13faeac24443b57a59e87260eb75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a5a7aecfd534279a23b643a1afee716"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d270f64c4bd94f628ae17ee988b914e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c522148140b14c2d984577f02b053542"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"class VAE(nn.Module):\n    def __init__(self, embed_dim, compress_dim, ff_dim):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.compress_dim = compress_dim\n\n        self.wc = nn.Linear(embed_dim, compress_dim, bias=True)\n        self.norm = nn.RMSNorm(compress_dim)\n        self.wuc = nn.Linear(compress_dim, ff_dim)\n        self.wuv = nn.Linear(compress_dim, ff_dim)\n        self.silu = nn.SiLU()\n        self.w_back = nn.Linear(ff_dim, embed_dim)\n\n    def forward(self, x, compressing=False):\n        x = self.wc(x)\n        if compressing: return x\n        return self.uncompress(x)\n\n    def uncompress(self, x):\n        x = self.norm(x)\n        return self.w_back(self.silu(self.wuc(x)) * self.wuv(x))\n\nclass Gate(nn.Module):\n    def __init__(self, embed_dim, dropout_rate=0.0):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.dropout_rate = dropout_rate\n        \n        self.gate = nn.Parameter(torch.ones(embed_dim)) # all from model embeddings first for stability\n\n    def forward(self, hidden, embed):\n        return embed * self.gate + (1 - self.gate) * hidden\n\n    def print_gates(self):\n        print(self.gate[:20])\n\n    def print_heatmap(self):\n        plt.imshow(self.gate.detach().cpu().numpy()[:20], cmap='hot', interpolation='nearest')\n        plt.colorbar()\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:13:38.996226Z","iopub.execute_input":"2025-04-02T09:13:38.996463Z","iopub.status.idle":"2025-04-02T09:13:39.003738Z","shell.execute_reply.started":"2025-04-02T09:13:38.996443Z","shell.execute_reply":"2025-04-02T09:13:39.003083Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# inject LoRA\npeft_config = LoraConfig(\n    task_type='CAUSAL_LM',\n    r=16,\n    lora_alpha=8,\n    target_modules=['q_proj', 'v_proj', 'k_proj', 'o_proj'],\n    lora_dropout=0.1\n)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\n# Gater\ngater = Gate(1536, 0.1)\n\n# load VAE\nvae = VAE(1536, 256, 7680)\nvae.load_state_dict(torch.load('/kaggle/input/vae-train/vae_epoch3.pt'))\n\nvae = vae.to(device)\ngater = gater.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:13:39.004438Z","iopub.execute_input":"2025-04-02T09:13:39.004702Z","iopub.status.idle":"2025-04-02T09:13:44.101768Z","shell.execute_reply.started":"2025-04-02T09:13:39.004682Z","shell.execute_reply":"2025-04-02T09:13:44.100797Z"}},"outputs":[{"name":"stdout","text":"trainable params: 4,358,144 || all params: 1,781,446,144 || trainable%: 0.2446\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-5-d587edc929ce>:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  vae.load_state_dict(torch.load('/kaggle/input/vae-train/vae_epoch3.pt'))\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# <think> and </think> and end_of_text mark\nsoth, eoth, eot = tokenizer('<think></think><｜end▁of▁sentence｜>').input_ids[1:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:13:44.102776Z","iopub.execute_input":"2025-04-02T09:13:44.103103Z","iopub.status.idle":"2025-04-02T09:13:44.107991Z","shell.execute_reply.started":"2025-04-02T09:13:44.103072Z","shell.execute_reply":"2025-04-02T09:13:44.106964Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"hidden_layer_num = 18\n\ndef cleanup():\n    gc.collect()\n    if device == 'cuda': torch.cuda.empty_cache()\n    elif device == 'mps': torch.cuda.empty_cache()\n\ndef tokenize(text, trn=True, max_length=1024, pad=False, device=device):\n    res = tokenizer(text, return_tensors='pt', truncation=trn, max_length=max_length, padding='max_length')\n    input_ids = res.input_ids.to(device)\n    attn_mask = res.attention_mask.to(device)\n    return input_ids, attn_mask\n\ndef sampler(problem, temperature=0.9, topk=16, max_length=2048, num=16, heating_steps=64):\n    model.eval()\n    vae.eval()\n    gater.eval()\n    \n    # tokenize\n    input_ids, attn_mask = tokenize(problem)\n    problem_len = input_ids.shape[1]\n\n    # prefill the problem\n    with torch.amp.autocast(device_type=str(device), dtype=torch.float16):\n        with torch.no_grad():\n            outputs = model(input_ids=input_ids, attention_mask=attn_mask, output_hidden_states=True, return_dict=True)\n\n    kv_cache = [tuple(tensor.expand(num, *(list(tensor.shape[1:]))) for tensor in layer) for layer in outputs.past_key_values]\n    last_hidden = outputs.hidden_states[hidden_layer_num].expand(num, -1, 1536)\n    hidden_cache = torch.Tensor(num, 0, 256).to(device)\n\n    text_end_appeared = False # if the first <｜end▁of▁sentence｜>\n    gen_all_done = False\n\n    text_end_mask = torch.ones(num, dtype=torch.int8).to(device)\n    text_end_indices = torch.ones(num, dtype=torch.long).to(device) * (-1)\n    \n    res = torch.zeros(num, 0, dtype=torch.long).to(device)\n    \n    for i in range(max_length):\n        try:\n            logits = outputs.logits[:, -1, :].float() # (num, vocab_size)\n            if i < 64: logits[:, eoth] = -1e6 # mask out the </think> token's prob -> 'heating up'\n\n            del outputs\n            cleanup()\n            \n            values, indices = torch.topk(logits, topk, largest=True, sorted=False, dim=-1)\n            probs = nn.functional.softmax(values / temperature, dim=-1)\n            if i == 0:\n                selected_choice = torch.torch.multinomial(probs[0], num_samples=1).view(-1).expand(num)\n                selected_index = indices.view(-1).gather(0, selected_choice).view(num, 1)\n            else:\n                selected_choice = torch.multinomial(probs.view(num, -1), num_samples=1)\n                selected_index = indices.gather(1, selected_choice)\n            res = torch.cat([res, selected_index], dim=1)\n            selected_index = selected_index.view(num)\n\n            if not gen_all_done and eot in selected_index:\n                text_end_appeared = True\n                text_end_mask.masked_fill_(selected_index == eot, 0)\n                text_end_indices.masked_fill_(selected_index == eot, i + problem_len)\n                gen_all_done = 1 in text_end_mask\n\n            if gen_all_done: break\n            \n            # forward\n            with torch.amp.autocast(device_type=str(device), dtype=torch.float16):\n                with torch.no_grad():\n                    hidden_cache = torch.cat([hidden_cache, vae(last_hidden[:, -1:, :], compressing=True)], dim=1)\n                    embeds = model.lm_head.weight[selected_index.view(num, 1).to('cuda:1')].to(device)\n                    embeds = gater(vae.uncompress(hidden_cache[:, -1:, :]), embeds)\n                    outputs = model(inputs_embeds=embeds, output_hidden_states=True, return_dict=True, use_cache=True, past_key_values=kv_cache)\n                    kv_cache = outputs.past_key_values\n            \n        except KeyboardInterrupt:\n            cleanup()\n            return res, hidden_cache, text_end_indices, input_ids\n\n    return res, hidden_cache, text_end_indices, input_ids\n\nboxed_match = re.compile(r'\\\\boxed\\{[^}]*\\}')\ndef verifier(model_anss, corr_ans):\n    res = []\n    corr_ans = parse(corr_ans)\n    for i in model_anss:\n        model_ans = boxed_match.findall(i)\n        if model_ans:\n            model_ans = parse(model_ans[-1])\n            res.append(1 if verify(model_ans, corr_ans) else -1)\n        else:\n            res.append(-1)\n    return res","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:17:26.088973Z","iopub.execute_input":"2025-04-02T09:17:26.089287Z","iopub.status.idle":"2025-04-02T09:17:26.103207Z","shell.execute_reply.started":"2025-04-02T09:17:26.089264Z","shell.execute_reply":"2025-04-02T09:17:26.102338Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"prompt = 'solve the math problem below, and put your ans in the \\boxed{}.\\n'\nproblem = 'Solve the equation: 2x + 1 = 5.<think>\\n'\n\nres, hidden_cache, text_end_indices, problem_input_ids = sampler(prompt + problem, num=16, topk=3, max_length=256)\nprint(tokenizer.batch_decode(res, ignore_special_tokens=True))\ncorrectness_rewards = torch.Tensor(verifier(tokenizer.batch_decode(res, ignore_special_tokens=True), '2')).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:17:29.535029Z","iopub.execute_input":"2025-04-02T09:17:29.535334Z","iopub.status.idle":"2025-04-02T09:18:22.243294Z","shell.execute_reply.started":"2025-04-02T09:17:29.535312Z","shell.execute_reply":"2025-04-02T09:18:22.242274Z"}},"outputs":[{"name":"stdout","text":"[\"First, we need to solve the equation 2x + 1 = 5.\\n\\nWe start by isolating the variable x. Subtracting 1 from both sides of the equation gives 2x = 4.\\n\\nNext, we divide both sides by 2 to find the value of x, resulting in x = 2.\\n\\nFinally, we present the solution in a clear and concise manner.\\n</think>\\n\\nTo solve the equation \\\\( 2x + 1 = 5 \\\\), we'll follow these steps:\\n\\n**Step 1:** Isolate the variable term.\\n\\nSubtract 1 from both sides to get the variable term by itself:\\n\\\\[\\n2x + 1 -\", 'First, we need to solve the equation 2x + 1 = 5 for x.\\n\\nFirst, subtract 1 from both sides to isolate the term with x:\\n2x = 4\\n\\nThen, divide both sides by 2 to get the value of x:\\nx = 2\\n\\nSo, the solution is x equals 2.\\n</think>\\n\\n**Step-by-Step Explanation:**\\n\\n1. **Given Equation:**\\n   \\n   Start with the equation:\\n   \\\\[\\n   2x + 1 = 5\\n   \\\\]\\n   \\n2. **Subtract 1 from Both Sides:**\\n   \\n   To isolate the term with \\\\( x \\\\), subtract ', 'Firstly, we need to solve the equation \\\\(2x + 1 = 5\\\\) for x.\\n\\nWe start by isolating the term with the variable. Subtract 1 from both sides to get \\\\(2x = 4\\\\).\\n\\nNext, divide both sides by 2 to find the value of x.\\n\\nThus, \\\\(x = 2\\\\).\\n</think>\\n\\n要解方程 \\\\(2x + 1 = 5\\\\)，我们可以按照以下步骤进行：\\n\\n1. **减去 1 从两边**：\\n   \\\\[\\n   2x + 1 - 1 = 5 - 1\\n   \\\\]\\n   这简化为：\\n   \\\\', \"First, to solve the equation 2x + 1 = 5, I need to isolate the variable x.\\n\\nFirst, I'll subtract 1 from both sides of the equation to maintain balance.\\n\\nThis gives 2x = 4.\\n\\nThen, I'll divide both sides by 2 to find the value of x.\\n\\nTherefore, x equals 2.\\n</think>\\n\\nTo solve the equation \\\\(2x + 1 = 5\\\\), we can follow these steps:\\n\\n1. **Subtract 1 from both sides** to isolate the term with the variable:\\n   \\\\[\\n   2x + 1 - 1 = 5 - 1\\n   \\\\]\\n\", 'Firstly, we need to solve the equation 2x + 1 = 5.\\n\\nTo do this, subtract 1 from both sides of the equation:\\n2x + 1 - 1 = 5 - 1\\nSimplify the equation:\\n2x = 4\\n\\nNext, divide both sides by 2 to solve for x:\\n2x / 2 = 4 / 2\\nx = 2\\n\\nTherefore, the solution is x = 2.\\n</think>\\n\\n**Step-by-Step Explanation:**\\n\\n1. **方程分析**：我们需要解方程 \\\\( 2x + 1 = 5 \\\\)。\\n\\n2. **移项', 'First, to solve the equation 2x + 1 = 5, I need to isolate the variable x. \\n\\nStep 1: Subtract 1 from both sides of the equation to eliminate the constant term on the left side.\\n2x + 1 = 5\\n2x = 5 - 1\\n2x = 4\\n\\nStep 2: Divide both sides by 2 to solve for x.\\nx = 4 / 2\\nx = 2\\n\\nTherefore, the solution to the equation is x = 2.\\n</think>\\n\\nTo solve the equation \\\\( 2x + 1 = 5 \\\\), follow these steps:\\n\\n1. **Sub', 'First, we need to solve for x in the equation 2x + 1 = 5.\\n\\nFirst, subtract 1 from both sides: 2x = 4.\\n\\nThen, divide both sides by 2: x = 2.\\n \\nThe solution is x equals 2.\\n```python\\n# Solve the equation 2x + 1 = 5\\nx = 2\\nprint(x)\\n```\\n\\n**答案**\\nx = 2\\n\\n```python\\nx = (5 - 1) / 2\\nprint(x)\\n```\\n\\n**答案**\\nx = 2\\n\\n```python\\nx = (5 - 1) / 2\\nprint', \"First, I need to solve the equation 2x + 1 = 5.\\n\\nFirst, I'll subtract 1 from both sides to isolate the term with x.\\n\\n2x = 4\\n\\nNext, I'll divide both sides by 2 to find the value of x.\\n\\nx = 2\\n\\nTherefore, the solution to the equation is x = 2.\\n</think>\\n\\nTo solve the equation \\\\(2x + 1 = 5\\\\), follow these steps:\\n\\n1. **Subtract 1 from both sides** to begin isolating the term with the variable:\\n   \\\\[\\n   2x + 1 - 1 = 5 - 1\\n  \", 'First, we need to solve the equation 2x + 1 = 5.\\n\\nTo find the value of x, we can subtract 1 from both sides of the equation:\\n2x = 5 - 1\\n\\nThen, simplify to get 2x = 4.\\n\\nFinally, divide both sides by 2 to solve for x:\\nx = 4 / 2\\nx = 2\\n</think>\\n\\n**Step-by-Step Explanation:**\\n\\n1. **Given Equation:**\\n   \\n   We start with the equation:\\n   \\\\[\\n   2x + 1 = 5\\n   \\\\]\\n\\n2. **Subtract 1 from Both Sides:**\\n', \"First, I need to solve the equation 2x + 1 = 5.\\n\\nFirst, I'll start by isolating the variable term.\\n\\nSubtract 1 from both sides to get:\\n2x = 5 - 1\\n\\nSimplify the right side:\\n2x = 4\\n\\nThen, divide both sides by 2 to solve for x:\\nx = 4 / 2\\n\\nFinally, simplify the fraction:\\nx = 2\\n\\nSo, the solution is x equals 2.\\n</think>\\n\\nTo solve the equation \\\\(2x + 1 = 5\\\\), follow these steps:\\n\\n1. **Subtract 1 from both sides** to eliminate the constant\", 'First, we have the equation 2x + 1 = 5.\\n\\nTo solve for x, we first subtract 1 from both sides:\\n2x + 1 = 5  \\nSubtract 1 from both sides:\\n2x = 4\\n\\nThen, we divide both sides by 2:\\nx = 2\\n\\nTherefore, the solution to the equation is x = 2.\\n</think>\\n\\nTo solve the equation \\\\( 2x + 1 = 5 \\\\), follow these steps:\\n\\n1. **Subtract 1 from both sides** to isolate the term with \\\\( x \\\\):\\n   \\\\[\\n   2x + 1 - 1 = 5', 'First, I need to solve the equation 2x + 1 = 5.\\n\\nFirst, subtract 1 from both sides to isolate the term with x.\\n\\n2x = 4\\n\\nThen, divide both sides by 2 to find x.\\nx = 2\\n\\nSo, the solution is x equals 2.\\n</think>\\n\\nTo solve the equation \\\\( 2x + 1 = 5 \\\\):\\n\\n1. Subtract 1 from both sides:\\n   \\\\[\\n   2x = 4\\n   \\\\]\\n\\n2. Divide both sides by 2:\\n   \\\\[\\n   x = 2\\n   \\\\]\\n\\n**Answer:** \\\\( x = 2 \\\\)<｜end▁of▁sentence｜>', \"Firstly, I need to solve the equation 2x + 1 = 5.\\n\\nTo find the value of x, I'll subtract 1 from both sides of the equation to isolate the term with x. This gives me 2x = 4.\\n\\nNext, I'll divide both sides by 2 to solve for x, resulting in x = 2.\\n\\nTherefore, the solution to the equation is x = 2.\\n</think>\\n\\n**Solution:**\\n\\nTo solve the equation \\\\(2x + 1 = 5\\\\):\\n\\n1. **Subtract 1 from both sides:**\\n   \\\\[\\n   2x + 1 - 1 = 5 -\", 'Firstly, to solve 2x + 1 = 5, we need to isolate x.\\n\\nWe start by subtracting 1 from both sides to get 2x = 4.\\n\\nThen, we divide both sides by 2 to find x = 2.\\n\\nSo, the solution is x equals 2.\\n</think>\\n\\n**Step-by-Step Explanation:**\\n\\n1. **方程解析：** 给定方程为 2x + 1 = 5，目标是求出 x 的值。\\n\\n2. **移项：** 首先，从方程两边减去 1，得到 2x = 4。\\n\\n3.', 'First, we need to solve the equation 2x + 1 = 5.\\n\\nTo find the value of x, we can subtract 1 from both sides:\\n\\n2x + 1 - 1 = 5 - 1\\nWhich simplifies to:\\n2x = 4\\n\\nThen, divide both sides by 2 to isolate x:\\n\\n2x / 2 = 4 / 2\\nSo, x = 2\\n</think>\\n\\n**Solution:**\\n\\nTo solve the equation \\\\(2x + 1 = 5\\\\), subtract 1 from both sides:\\n\\n\\\\[\\n2x = 4\\n\\\\]\\n\\nThen, divide both sides by 2:\\n\\n\\\\', 'First, to solve the equation 2x + 1 =5, we need to isolate the variable x.\\n\\nFirst, subtract 1 from both sides of the equation to eliminate the constant term.\\n\\n2x +1 -1 =5 -1\\nSimplify:\\n2x =4\\n\\nNext, divide both sides by 2 to solve for x.\\n\\n2x/2 =4/2\\nSimplify:\\nx=2\\n\\nTherefore, the solution to the equation is x=2.\\n</think>\\n\\nTo solve the equation \\\\(2x + 1 = 5\\\\), follow these steps:\\n\\n1. **Subtract 1 from both sides** to isolate the term with the variable']\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from datasets import load_dataset\ndata = load_dataset('open-r1/OpenR1-Math-220k', split='train')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:18:53.896968Z","iopub.execute_input":"2025-04-02T09:18:53.897261Z","iopub.status.idle":"2025-04-02T09:19:20.618264Z","shell.execute_reply.started":"2025-04-02T09:18:53.897240Z","shell.execute_reply":"2025-04-02T09:19:20.617607Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.13k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4478045df66a4391a0c11ce5c5bdf3e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c483fc33e02f48eea81c6cdb0646d2f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00010.parquet:   0%|          | 0.00/214M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e50f52bb167e4401b85097debc5e1ed8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00010.parquet:   0%|          | 0.00/215M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d9166e8a1614448a0459de0e7c9debb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00010.parquet:   0%|          | 0.00/215M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64f487187f2b44c3a11fd77de6cfbf2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00003-of-00010.parquet:   0%|          | 0.00/217M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3945da4df0e047aca416feb3ff956cae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00004-of-00010.parquet:   0%|          | 0.00/215M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c5b98a9188b429caea88e37d8c414e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00005-of-00010.parquet:   0%|          | 0.00/214M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00f3c1459208430bb30befdfad991226"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00006-of-00010.parquet:   0%|          | 0.00/216M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed5e950bf796486a8a3893ae195558a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00007-of-00010.parquet:   0%|          | 0.00/216M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cca645544784738bf5c0cc69ec756dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00008-of-00010.parquet:   0%|          | 0.00/214M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94f12e799a3f4325a40e7dcd336406e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00009-of-00010.parquet:   0%|          | 0.00/215M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4eb19338d1f4e21b36d281f817d6af3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/93733 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7e51dbf69f0431cb4dee4bd7a68b801"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"optimizers = [AdamW(model.parameters(), lr=3e-5), AdamW(vae.parameters(), lr=5e-5), Adam(gater.parameters(), lr=1e-3)]\nscaler = torch.amp.GradScaler(device=device)\nlossf = nn.CrossEntropyLoss(reduction='none')\n\ndef save_model(steps):\n    model.save_pretrained(f'./model-{steps}')\n    torch.save(vae.state_dict(), f'vae-{steps}.pt')\n    torch.save(gater.state_dict(), f'gater-{steps}.pt')\n\ndef step_optimizer():\n    for i in optimizers:\n        scaler.step(i)\n    scaler.update()\n\ndef zero_grad_optimizer():\n    for i in optimizers:\n        i.zero_grad(set_to_none=True)\n\nnum_epochs = 2 # for each RL batch\ntotal_epochs = 1 # on the whole data\ngradient_accumulation_steps = 64\nlog_interval = 1\nsave_interval = 64\nbatch_size = 1\nmax_train_length = 1024\nmax_sample_length = 16\nsample_num = 16\nsample_topk = 10\n\nstep = 1 # total step count\n\nfor total_epoch in range(total_epochs):\n    for row in data:\n        problem, ans = row['problem'], row['answer']\n        res, hidden_cache, text_end_indices, input_ids = sampler(prompt + problem, num=sample_num, topk=sample_topk, max_length=max_sample_length)\n        \n        correctness_rewards = torch.Tensor(verifier(tokenizer.batch_decode(res, ignore_special_tokens=True), ans)).to(device)\n        len_rewards = text_end_indices.float()\n        \n        # TODO: check the accuracy to determine whether to further sample\n\n        # normalization\n        correctness_rewards -= correctness_rewards.mean()\n        len_rewards -= len_rewards.mean()\n        correctness_rewards /= ((correctness_rewards ** 2).sum() ** 0.5 + 1e-6)\n        len_rewards /= (torch.abs(len_rewards.max()) + 1e-6)\n        print(correctness_rewards, len_rewards)\n\n        # training\n        model.train()\n        vae.train()\n        gater.train()\n        for epoch in range(num_epochs):\n            if res.shape[1] > max_train_length:\n                seqs = torch.cat([input_ids.expand(sample_num, -1), res[:, :max_train_length]], dim=1)\n            else:\n                seqs = torch.cat([input_ids.expand(sample_num, -1), res], dim=1)\n            # build mask\n            mask_ = torch.arange(0, seqs.shape[1], dtype=torch.long).expand(sample_num, -1)\n            mask = torch.zeros(1, input_ids.shape[1]).expand(sample_num, -1)\n            mask.masked_fill_(mask_ <= text_end_indices, 1)\n            del mask_\n            for i in range(0, sample_num, batch_size):\n                try:\n                    cleanup()\n                    embeds = model.lm_heads[seqs[i:i + batch_size], :-1]\n                    hidden_cache_slice = hidden_cache[i:i + batch_size]\n                    with torch.amp.autocast(device_type=str(device), dtype=torch.float16):\n                        embeds[:, input_ids.shape[1]] = gater(hidden_cache_slice, embeds[:, input_ids.shape[1]])\n                        outputs = model(inputs_embeds=embeds, attention_mask=mask, output_hidden_states=True, return_dict=True)\n                    loss = lossf(output.logits.transpose(1, 2), seqs[i:i + batch_size, 1:].masked_fill(mask == 0, -100))\n                    hidden = outputs.hidden_states[hidden_layer_num]\n                    loss = (loss.sum(dim=-1) * (correctness_rewards + len_rewards)).mean() / (text_end_indices + 1).sum()\n                    del outputs; cleanup()\n                    \n                    scaler.scale(loss).backward()\n\n                    if step % gradient_accumulation_steps == 0:\n                        step_optimizer()\n                        zero_grad_optimizer()\n\n                    if step % (gradient_accumulation_steps * log_interval) == 0:\n                        print(f\"Step {step}, Loss: {loss.item():.3f}\")\n\n                except KeyboardInterrupt:\n                    cleanup()\n         \n    # Save checkpoint\n    if step % save_interval == 0:\n        save_model(step)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:25:27.178785Z","iopub.execute_input":"2025-04-02T09:25:27.179119Z","iopub.status.idle":"2025-04-02T09:25:33.600496Z","shell.execute_reply.started":"2025-04-02T09:25:27.179092Z","shell.execute_reply":"2025-04-02T09:25:33.599409Z"}},"outputs":[{"name":"stdout","text":"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0') tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0')\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-2ffa11731875>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mmask_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtext_end_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mmask_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1040) must match the size of tensor b (16) at non-singleton dimension 1"],"ename":"RuntimeError","evalue":"The size of tensor a (1040) must match the size of tensor b (16) at non-singleton dimension 1","output_type":"error"}],"execution_count":24}]}