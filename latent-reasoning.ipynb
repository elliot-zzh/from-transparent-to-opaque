{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":230366006,"sourceType":"kernelVersion"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install math-verify[antlr4_13_2]\n!pip install antlr4-python3-runtime==4.13.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T14:02:06.422935Z","iopub.execute_input":"2025-04-03T14:02:06.423223Z","iopub.status.idle":"2025-04-03T14:02:15.171183Z","shell.execute_reply.started":"2025-04-03T14:02:06.423197Z","shell.execute_reply":"2025-04-03T14:02:15.170148Z"}},"outputs":[{"name":"stdout","text":"Collecting math-verify[antlr4_13_2]\n  Downloading math_verify-0.7.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting latex2sympy2_extended==1.10.1 (from math-verify[antlr4_13_2])\n  Downloading latex2sympy2_extended-1.10.1-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from latex2sympy2_extended==1.10.1->math-verify[antlr4_13_2]) (1.13.1)\nRequirement already satisfied: antlr4-python3-runtime<=4.13.2,>=4.9.3 in /usr/local/lib/python3.10/dist-packages (from latex2sympy2_extended==1.10.1->math-verify[antlr4_13_2]) (4.9.3)\nCollecting antlr4-python3-runtime<=4.13.2,>=4.9.3 (from latex2sympy2_extended==1.10.1->math-verify[antlr4_13_2])\n  Downloading antlr4_python3_runtime-4.13.2-py3-none-any.whl.metadata (304 bytes)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->latex2sympy2_extended==1.10.1->math-verify[antlr4_13_2]) (1.3.0)\nDownloading latex2sympy2_extended-1.10.1-py3-none-any.whl (207 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading antlr4_python3_runtime-4.13.2-py3-none-any.whl (144 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading math_verify-0.7.0-py3-none-any.whl (28 kB)\nInstalling collected packages: antlr4-python3-runtime, latex2sympy2_extended, math-verify\n  Attempting uninstall: antlr4-python3-runtime\n    Found existing installation: antlr4-python3-runtime 4.9.3\n    Uninstalling antlr4-python3-runtime-4.9.3:\n      Successfully uninstalled antlr4-python3-runtime-4.9.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nomegaconf 2.3.0 requires antlr4-python3-runtime==4.9.*, but you have antlr4-python3-runtime 4.13.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed antlr4-python3-runtime-4.13.2 latex2sympy2_extended-1.10.1 math-verify-0.7.0\nRequirement already satisfied: antlr4-python3-runtime==4.13.2 in /usr/local/lib/python3.10/dist-packages (4.13.2)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW, Adam\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport datasets\nfrom peft import get_peft_model, LoraConfig\n\nimport gc\nimport re\nimport threading\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n\nimport polars as pl\nimport matplotlib.pyplot as plt\n\nfrom math_verify import parse, verify","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T14:02:15.172270Z","iopub.execute_input":"2025-04-03T14:02:15.172580Z","iopub.status.idle":"2025-04-03T14:02:36.453125Z","shell.execute_reply.started":"2025-04-03T14:02:15.172549Z","shell.execute_reply":"2025-04-03T14:02:36.452354Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"data_raw = pl.scan_parquet('hf://datasets/open-r1/OpenR1-Math-220k/data/train-*.parquet') # lazy load\nmodel_name = 'deepseek-ai/Deepseek-R1-Distill-Qwen-1.5B'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype=torch.float16, attn_implementation='sdpa')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T14:02:36.453974Z","iopub.execute_input":"2025-04-03T14:02:36.454640Z","iopub.status.idle":"2025-04-03T14:02:57.364290Z","shell.execute_reply.started":"2025-04-03T14:02:36.454605Z","shell.execute_reply":"2025-04-03T14:02:57.363578Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e2f511c78c24fd2a6be41cfeee2e944"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26ca612941a34d00adf333d042740030"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10f1f4db5e8e4c8b9b3f533aa894b7ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7ae4856a83e4079ab10bfa94a46f63e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c11264b9c0e44da8973cfe8877f8d267"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"class VAE(nn.Module):\n    def __init__(self, embed_dim, compress_dim, ff_dim):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.compress_dim = compress_dim\n\n        self.wc = nn.Linear(embed_dim, compress_dim, bias=True)\n        self.norm = nn.RMSNorm(compress_dim)\n        self.wuc = nn.Linear(compress_dim, ff_dim)\n        self.wuv = nn.Linear(compress_dim, ff_dim)\n        self.silu = nn.SiLU()\n        self.w_back = nn.Linear(ff_dim, embed_dim)\n\n    def forward(self, x, compressing=False):\n        x = self.wc(x)\n        if compressing: return x\n        return self.uncompress(x)\n\n    def uncompress(self, x):\n        x = self.norm(x)\n        return self.w_back(self.silu(self.wuc(x)) * self.wuv(x))\n\nclass Gate(nn.Module):\n    def __init__(self, embed_dim, dropout_rate=0.0):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.dropout_rate = dropout_rate\n        \n        self.gate = nn.Parameter(torch.ones(embed_dim)) # all from model embeddings first for stability\n\n    def forward(self, hidden, embed):\n        return embed * self.gate + (1 - self.gate) * hidden\n\n    def print_gates(self):\n        print(self.gate[:20])\n\n    def print_heatmap(self):\n        plt.imshow(self.gate.detach().cpu().numpy()[:20], cmap='hot', interpolation='nearest')\n        plt.colorbar()\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T14:02:57.365102Z","iopub.execute_input":"2025-04-03T14:02:57.365400Z","iopub.status.idle":"2025-04-03T14:02:57.372977Z","shell.execute_reply.started":"2025-04-03T14:02:57.365370Z","shell.execute_reply":"2025-04-03T14:02:57.372322Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# inject LoRA\npeft_config = LoraConfig(\n    task_type='CAUSAL_LM',\n    r=16,\n    lora_alpha=8,\n    target_modules=['q_proj', 'v_proj', 'k_proj', 'o_proj'],\n    lora_dropout=0.1\n)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\n# Gater\ngater = Gate(1536, 0.1)\n\n# load VAE\nvae = VAE(1536, 256, 7680)\nvae.load_state_dict(torch.load('/kaggle/input/vae-train/vae_epoch3.pt'))\n\nvae = vae.to(device)\ngater = gater.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T14:02:57.373664Z","iopub.execute_input":"2025-04-03T14:02:57.373970Z","iopub.status.idle":"2025-04-03T14:03:05.695491Z","shell.execute_reply.started":"2025-04-03T14:02:57.373942Z","shell.execute_reply":"2025-04-03T14:03:05.694796Z"}},"outputs":[{"name":"stdout","text":"trainable params: 4,358,144 || all params: 1,781,446,144 || trainable%: 0.2446\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-5-d587edc929ce>:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  vae.load_state_dict(torch.load('/kaggle/input/vae-train/vae_epoch3.pt'))\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# <think> and </think> and end_of_text mark\nsoth, eoth, eot = tokenizer('<think></think><｜end▁of▁sentence｜>').input_ids[1:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T14:03:05.697663Z","iopub.execute_input":"2025-04-03T14:03:05.697936Z","iopub.status.idle":"2025-04-03T14:03:05.702529Z","shell.execute_reply.started":"2025-04-03T14:03:05.697905Z","shell.execute_reply":"2025-04-03T14:03:05.701595Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"hidden_layer_num = 18\n\ndef cleanup():\n    gc.collect()\n    if device == 'cuda': torch.cuda.empty_cache()\n    elif device == 'mps': torch.cuda.empty_cache()\n\ndef tokenize(text, direct=False, max_length=1024, pad=False, device=device):\n    if direct:\n        res =tokenizer(text, return_tensors='pt')\n    else:\n        res = tokenizer(text, return_tensors='pt', truncation=trn, max_length=max_length, padding='max_length')\n    input_ids = res.input_ids.to(device)\n    attn_mask = res.attention_mask.to(device)\n    return input_ids, attn_mask\n\ndef sampler(problem, temperature=0.9, topk=16, max_length=2048, num=16, heating_steps=64):\n    model.eval()\n    vae.eval()\n    gater.eval()\n    \n    # tokenize\n    input_ids, attn_mask = tokenize(problem, direct=True)\n    problem_len = input_ids.shape[1]\n\n    # prefill the problem\n    with torch.amp.autocast(device_type=str(device), dtype=torch.float16):\n        with torch.no_grad():\n            outputs = model(input_ids=input_ids, attention_mask=attn_mask, output_hidden_states=True, return_dict=True)\n\n    kv_cache = [tuple(tensor.expand(num, *(list(tensor.shape[1:]))) for tensor in layer) for layer in outputs.past_key_values]\n    last_hidden = outputs.hidden_states[hidden_layer_num].expand(num, -1, 1536)\n    hidden_cache = torch.Tensor(num, 0, 256).to(device)\n\n    text_end_appeared = False # if the first <｜end▁of▁sentence｜>\n    gen_all_done = False\n\n    text_end_mask = torch.ones(num, dtype=torch.int8).to(device)\n    text_end_indices = torch.ones(num, dtype=torch.long).to(device) * max_length\n    \n    res = torch.zeros(num, 0, dtype=torch.long).to(device)\n    \n    for i in range(max_length):\n        try:\n            logits = outputs.logits[:, -1, :].float() # (num, vocab_size)\n            if i < 64: logits[:, eoth] = -1e6 # mask out the </think> token's prob -> 'heating up'\n\n            del outputs\n            cleanup()\n            \n            values, indices = torch.topk(logits, topk, largest=True, sorted=False, dim=-1)\n            probs = nn.functional.softmax(values / temperature, dim=-1)\n            if i == 0:\n                selected_choice = torch.torch.multinomial(probs[0], num_samples=1).view(-1).expand(num)\n                selected_index = indices.view(-1).gather(0, selected_choice).view(num, 1)\n            else:\n                selected_choice = torch.multinomial(probs.view(num, -1), num_samples=1)\n                selected_index = indices.gather(1, selected_choice)\n            res = torch.cat([res, selected_index], dim=1)\n            selected_index = selected_index.view(num)\n\n            if not gen_all_done and eot in selected_index:\n                text_end_appeared = True\n                text_end_mask.masked_fill_(selected_index == eot, 0)\n                text_end_indices.masked_fill_(selected_index == eot, i + problem_len)\n                gen_all_done = 1 in text_end_mask\n\n            if gen_all_done: break\n            \n            # forward\n            with torch.amp.autocast(device_type=str(device), dtype=torch.float16):\n                with torch.no_grad():\n                    hidden_cache = torch.cat([hidden_cache, vae(last_hidden[:, -1:, :], compressing=True)], dim=1)\n                    embeds = model.lm_head.weight[selected_index.view(num, 1).to('cuda:1')].to(device)\n                    embeds = gater(vae.uncompress(hidden_cache[:, -1:, :]), embeds)\n                    outputs = model(inputs_embeds=embeds, output_hidden_states=True, return_dict=True, use_cache=True, past_key_values=kv_cache)\n                    kv_cache = outputs.past_key_values\n            \n        except KeyboardInterrupt:\n            cleanup()\n            return res, hidden_cache, text_end_indices, input_ids\n\n    return res, hidden_cache, text_end_indices, input_ids\n\nboxed_match = re.compile(r'\\\\boxed\\{[^}]*\\}')\ndef verifier(model_anss, corr_ans):\n    res = []\n    corr_ans = parse(corr_ans)\n    for i in model_anss:\n        model_ans = boxed_match.findall(i)\n        if model_ans:\n            model_ans = parse(model_ans[-1])\n            res.append(1 if verify(model_ans, corr_ans) else -1)\n        else:\n            res.append(-1)\n    return res","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T14:23:33.267342Z","iopub.execute_input":"2025-04-03T14:23:33.267724Z","iopub.status.idle":"2025-04-03T14:23:33.283020Z","shell.execute_reply.started":"2025-04-03T14:23:33.267692Z","shell.execute_reply":"2025-04-03T14:23:33.282131Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"prompt = 'solve the math problem below, and put your ans in the \\boxed{}.\\n'\nproblem = 'Solve the equation: 2x + 1 = 5.<think>\\n'\n\nres, hidden_cache, text_end_indices, problem_input_ids = sampler(prompt + problem, num=16, topk=3, max_length=256)\nprint(tokenizer.batch_decode(res, ignore_special_tokens=True))\ncorrectness_rewards = torch.Tensor(verifier(tokenizer.batch_decode(res, ignore_special_tokens=True), '2')).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T14:03:05.720279Z","iopub.execute_input":"2025-04-03T14:03:05.720487Z","iopub.status.idle":"2025-04-03T14:03:58.609345Z","shell.execute_reply.started":"2025-04-03T14:03:05.720469Z","shell.execute_reply":"2025-04-03T14:03:58.608440Z"}},"outputs":[{"name":"stderr","text":"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n","output_type":"stream"},{"name":"stdout","text":"[\"First, I need to solve the equation 2x + 1 = 5.\\n\\nI'll start by subtracting 1 from both sides to isolate the term with x.\\n\\nThis gives 2x = 4.\\n\\nThen, I'll divide both sides by 2 to solve for x.\\n\\nSo, x equals 2.\\n</think>\\n\\n要解方程式 2x + 1 = 5，可以按照以下步骤进行：\\n\\n1. 从两边减去 1：\\n   \\\\[\\n   2x + 1 - 1 = 5 - 1\\n   \\\\]\\n   简化后得到：\\n   \\\\[\\n   2x = 4\\n\", 'First, we need to solve the equation step by step.\\n\\nFirst, we can subtract 1 from both sides to isolate the term with x.\\n\\n2x + 1 = 5\\nSubtract 1 from both sides:\\n2x = 4\\n\\nNext, we can divide both sides by 2 to solve for x.\\n\\n2x / 2 = 4 / 2\\nx = 2\\n\\nSo, the solution to the equation is x = 2.\\n</think>\\n\\nTo solve the equation \\\\(2x + 1 = 5\\\\), follow these steps:\\n\\n1. Start with the original equation:\\n   \\\\[\\n   2x + 1 = 5\\n  ', \"First, I need to solve for x in the equation 2x + 1 = 5.\\n\\nTo do that, I'll start by subtracting 1 from both sides of the equation.\\n\\nSubtracting 1 from 2x gives 2x, and subtracting 1 from 5 gives 4.\\n\\nSo now the equation is 2x = 4.\\n\\nNext, I need to isolate x by dividing both sides of the equation by 2.\\n\\nDividing 2x by 2 gives x, and dividing 4 by 2 gives 2.\\n\\nTherefore, the solution to the equation is x = 2.\\n</think>\\n\\nTo solve the equation \\\\(2x +\", 'First, we need to solve the equation 2x + 1 = 5.\\n\\nWe can start by isolating the variable term. Subtract 1 from both sides to get 2x = 4.\\n\\nThen, divide both sides by 2 to solve for x, which gives x = 2.\\n\\nFinally, we can verify the solution by substituting x = 2 back into the original equation to confirm it holds true.\\n</think>\\n\\nTo solve the equation \\\\(2x + 1 = 5\\\\), follow these steps:\\n\\n1. **Subtract 1 from both sides**:\\n   \\\\[\\n   2x + 1 - 1 = 5 - 1\\n', 'First, we have the equation 2x + 1 = 5.\\n\\nTo solve for x, we first need to isolate the term with x.\\n\\nSubtract 1 from both sides to get 2x = 4.\\n\\nThen, divide both sides by 2 to find x = 2.\\n</s>\\n</think>\\n\\nTo solve the equation 2x + 1 = 5:\\n\\n1. Subtract 1 from both sides: 2x = 4\\n2. Divide both sides by 2: x = 2\\n\\n**Step-by-step explanation:**\\n\\n- **Step 1:** Start with the equation 2x + 1 = 5.\\n- **Step', 'First, we have the equation 2x + 1 = 5.\\n\\nTo solve for x, we first subtract 1 from both sides to get 2x = 4.\\n\\nThen, we divide both sides by 2 to find that x = 2.\\n\\nTherefore, the solution is x equals 2.\\n</think>\\n\\n解方程：2x + 1 = 5.\\n\\n**步骤解析：**\\n\\n1. **移项**：将常数项1移到等号右边。\\n   \\n   \\\\ 2x = 5 - 1\\n   \\n   \\\\ 2x = 4\\n\\n2. **解x**：两边同时除以2。\\n   \\n   x =', 'First, I will solve the equation 2x + 1 = 5.\\n\\nFirst, I will subtract 1 from both sides to get 2x = 4.\\n\\nThen, I will divide both sides by 2 to find that x = 2.\\n \\nFinally, I will verify the solution by substituting 2 for x in the original equation to confirm that it holds true.\\n</think>\\n\\n**Solution**\\n\\nTo solve the equation 2x + 1 = 5, follow these steps:\\n1. Subtract 1 from both sides:\\n   2x + 1 - 1 = 5 - 1\\n   Simplifying:\\n   2x = 4\\n', \"First, I need to solve the equation 2x + 1 = 5.\\n\\nFirst, I'll subtract 1 from both sides to isolate the term with x.\\n\\n2x + 1 - 1 = 5 - 1\\nSimplifying, I get 2x = 4.\\n\\nNext, I'll divide both sides by 2 to solve for x.\\n\\n2x / 2 = 4 / 2\\nSimplifying further, I find that x = 2.\\n\\nTherefore, the solution is x equals 2.\\n</think>\\n\\n**Step-by-Step Explanation:**\\n\\n1. **Given Equation:**\\n   \\\\[\\n   2x + 1 =\", 'First, I need to solve the equation 2x + 1 = 5.\\n\\nTo find the value of x, I\\'ll subtract 1 from both sides of the equation.\\n\\nThis simplifies to 2x = 4.\\n\\nThen, I\\'ll divide both sides by 2, resulting in x = 2.\\n\\nSo, the solution is x equals 2.\\n\\n```java\\nimport java.util.Map;\\nimport java.util.Set;\\n\\npublic class EquationSolver {\\n    public static Map<String, Integer> solveEquation(String equation) {\\n        // 分析方程，提取变量和系数\\n        Map<Integer, String> variables = Map.of(\"x\", \"2\").put(\"x\");\\n', 'First, we need to solve the equation 2x + 1 = 5.\\n\\nWe start by isolating the variable term. Subtract 1 from both sides to get rid of the constant term on the left side:\\n2x = 4\\n\\nThen, divide both sides by 2 to solve for x:\\nx = 2\\n\\nSo, the solution to the equation is x equals 2.\\n</think>\\n\\nTo solve the equation \\\\(2x + 1 = 5\\\\), follow these steps:\\n\\n1. **Isolate the term with the variable (x):**\\n   Subtract 1 from both sides of the equation:\\n   \\\\[\\n   2x + 1 - 1', \"Firstly, I need to solve the equation 2x + 1 = 5.\\n\\nI'll start by isolating the variable term. Subtract 1 from both sides to get 2x = 4.\\n\\nThen, I'll divide both sides by 2 to find that x = 2.\\n\\nFinally, I will verify the solution by substituting x = 2 back into the original equation to ensure it satisfies the equation.\\n</think>\\n\\n要解方程：2x + 1 = 5\\n\\n**步骤解析：**\\n\\n1. **减去1**：将方程两边都减去1，得到：\\n   \\\\[\\n   2x = 4\\n   \\\\\", 'First, we have the equation 2x + 1 = 5.\\n\\nTo solve for x, we need to isolate the variable.\\n\\nFirst, subtract 1 from both sides of the equation to get 2x = 4.\\n\\nThen, divide both sides by 2 to find x = 2.\\n\\nTherefore, the solution is x equals 2.\\n</think>\\n\\nTo solve the equation \\\\(2x + 1 = 5\\\\):\\n\\n1. **Subtract 1 from both sides**:\\n   \\\\[\\n   2x = 5 - 1\\n   \\\\]\\n   \\\\[\\n   2x = 4\\n   \\\\]\\n\\n2. **Divide both sides by', \"First, I need to solve the equation 2x + 1 = 5.\\n\\nI'll start by isolating the variable term. Subtract 1 from both sides to get 2x = 4.\\n\\nNext, divide both sides by 2 to solve for x.\\n\\nSo, x equals 2.\\n\\nFinally, I'll verify the solution by substituting x=2 back into the original equation to ensure it holds true.\\n</think>\\n\\n**Solution:**\\n\\nTo solve the equation \\\\(2x + 1 = 5\\\\), follow these steps:\\n\\n1. **Subtract 1 from both sides** to isolate the variable term:\\n   \\\\[\\n   2x + 1 - \", \"First, I'll write down the equation: 2x + 1 = 5.\\n\\nThen, I'll subtract 1 from both sides to get: 2x = 4.\\n\\nNext, I'll divide both sides by 2 to solve for x, resulting in: x = 2.\\n\\nFinally, I'll present the solution.\\n</think>\\n\\nTo solve the equation 2x + 1 = 5:\\n\\n1. Subtract 1 from both sides:\\n   2x + 1 - 1 = 5 - 1\\n   2x = 4\\n\\n2. Divide both sides by 2:\\n   x = 4 / 2\\n   x =\", 'First, we need to solve the equation 2x + 1 = 5.\\n\\nTo find the value of x, we first subtract 1 from both sides of the equation to isolate the term with the variable.\\n\\nThis simplifies the equation to 2x = 4.\\n\\nFinally, we divide both sides by 2 to solve for x, which gives us x = 2.\\n</think>\\n\\nTo solve the equation \\\\(2x + 1 = 5\\\\):\\n\\n1. Subtract 1 from both sides:\\n   \\\\(2x = 4\\\\)\\n\\n2. Divide both sides by 2:\\n   \\\\(x = 2\\\\)\\n\\n**Answer:** \\\\(x = 2\\\\)<｜end▁of▁sentence｜>', \"First, we need to solve the equation 2x + 1 = 5.\\n\\nWe can start by subtracting 1 from both sides to isolate the term with the variable.\\n\\nThis gives us 2x = 4.\\n\\nThen, divide both sides by 2 to solve for x.\\nx = 2.\\n\\nFinally, we'll present the solution.\\n</think>\\n\\nTo solve the equation \\\\(2x + 1 = 5\\\\), follow these steps:\\n\\n1. Subtract 1 from both sides:\\n   \\\\[\\n   2x + 1 - 1 = 5 - 1\\n   \\\\]\\n   \\\\[\\n   2x = 4\\n   \\\\]\\n\\n2\"]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from datasets import load_dataset\ndata = load_dataset('open-r1/OpenR1-Math-220k', split='train')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T14:03:58.610359Z","iopub.execute_input":"2025-04-03T14:03:58.610686Z","iopub.status.idle":"2025-04-03T14:04:24.981769Z","shell.execute_reply.started":"2025-04-03T14:03:58.610654Z","shell.execute_reply":"2025-04-03T14:04:24.981129Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.13k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fca0c76278b4cadba16ddbbf05180da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2ca61250f9546d59de583b6c3481a80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00010.parquet:   0%|          | 0.00/214M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7f89a6d62384ebb8267b6eb65252ca1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00010.parquet:   0%|          | 0.00/215M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0df1a3704a6e4170947a02c879279217"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00010.parquet:   0%|          | 0.00/215M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"106a3db4f52c4b12b56b3e13c06b2227"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00003-of-00010.parquet:   0%|          | 0.00/217M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2e3516003e4480d8b45f8cc2b9fcd63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00004-of-00010.parquet:   0%|          | 0.00/215M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d722931956f42c6b18d1ecb50604857"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00005-of-00010.parquet:   0%|          | 0.00/214M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2b7d0bc59474e379639ffccf3f68a09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00006-of-00010.parquet:   0%|          | 0.00/216M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"450cbd1a53b444f390d00649f363ecfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00007-of-00010.parquet:   0%|          | 0.00/216M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"790b41d6aecd449d98b881673a32358c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00008-of-00010.parquet:   0%|          | 0.00/214M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62fe949954f94bfe9baa44a946c4e8ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00009-of-00010.parquet:   0%|          | 0.00/215M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84186c3122024690a4caecfcdc130db0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/93733 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d7ac461c5ce42c88ac821bcd5f7795c"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"optimizers = [AdamW(model.parameters(), lr=3e-5), AdamW(vae.parameters(), lr=5e-5), Adam(gater.parameters(), lr=1e-3)]\nscaler = torch.amp.GradScaler(device=device)\nlossf = nn.CrossEntropyLoss(reduction='none')\n\ndef save_model(steps):\n    model.save_pretrained(f'./model-{steps}')\n    torch.save(vae.state_dict(), f'vae-{steps}.pt')\n    torch.save(gater.state_dict(), f'gater-{steps}.pt')\n\ndef step_optimizer():\n    for i in optimizers:\n        scaler.step(i)\n    scaler.update()\n\ndef zero_grad_optimizer():\n    for i in optimizers:\n        i.zero_grad(set_to_none=True)\n\nnum_epochs = 2 # for each RL batch\ntotal_epochs = 1 # on the whole data\ngradient_accumulation_steps = 64\nlog_interval = 1\nsave_interval = 64\nbatch_size = 1\nmax_train_length = 1024\nmax_sample_length = 8\nsample_num = 16\nsample_topk = 10\n\nstep = 1 # total step count\n\nfor total_epoch in range(total_epochs):\n    for row in data:\n        problem, ans = row['problem'], row['answer']\n        res, hidden_cache, text_end_indices, input_ids = sampler(prompt + problem, num=sample_num, topk=sample_topk, max_length=max_sample_length)\n        if hidden_cache.shape[1] == max_sample_length: hidden_cache = hidden_cache[:, :-1]\n        \n        correctness_rewards = torch.Tensor(verifier(tokenizer.batch_decode(res, ignore_special_tokens=True), ans)).to(device)\n        len_rewards = text_end_indices.float()\n        \n        # TODO: check the accuracy to determine whether to further sample\n\n        # normalization\n        correctness_rewards -= correctness_rewards.mean()\n        len_rewards -= len_rewards.mean()\n        correctness_rewards /= ((correctness_rewards ** 2).sum() ** 0.5 + 1e-6)\n        len_rewards /= (torch.abs(len_rewards.max()) + 1e-6)\n        print(correctness_rewards, len_rewards, sep='\\n')\n\n        # training\n        model.train()\n        vae.train()\n        gater.train()\n        for epoch in range(num_epochs):\n            if res.shape[1] > max_train_length:\n                seqs = torch.cat([input_ids.expand(sample_num, -1), res[:, :max_train_length]], dim=1)\n            else:\n                seqs = torch.cat([input_ids.expand(sample_num, -1), res], dim=1)\n            # build mask\n            mask_ = torch.arange(0, seqs.shape[1] - 1, dtype=torch.long).expand(sample_num, -1).to(device)\n            mask = torch.zeros(1, seqs.shape[1] - 1).expand(sample_num, -1).to(device)\n            mask = mask.masked_fill(mask_ <= text_end_indices.unsqueeze(1), 1)\n            del mask_\n            for i in range(0, sample_num, batch_size):\n                try:\n                    cleanup()\n                    embeds = model.lm_head.weight[seqs[i:i + batch_size].to('cuda:1')][:, :-1].to('cuda:0')\n                    hidden_cache_slice = hidden_cache[i:i + batch_size]\n                    with torch.amp.autocast(device_type=str(device), dtype=torch.float16):\n                        new_embeds = embeds.clone()\n                        new_embeds[:, input_ids.shape[1]:] = gater(vae.uncompress(hidden_cache_slice), embeds[:, input_ids.shape[1]:])\n                        outputs = model(inputs_embeds=new_embeds, attention_mask=mask[i:i + batch_size], output_hidden_states=True, return_dict=True)\n                    loss = lossf(outputs.logits.transpose(1, 2), seqs[i:i + batch_size, 1:].masked_fill(mask[i:i + batch_size] == 0, -100))\n                    hidden = outputs.hidden_states[hidden_layer_num]\n                    loss = (loss.sum(dim=-1) * (correctness_rewards + len_rewards)).mean() / (text_end_indices + 1).sum()\n                    del outputs; cleanup()\n                    \n                    scaler.scale(loss).backward()\n\n                    if step % gradient_accumulation_steps == 0:\n                        step_optimizer()\n                        zero_grad_optimizer()\n\n                    if step % (gradient_accumulation_steps * log_interval) == 0:\n                        print(f\"Step {step}, Loss: {loss.item():.3f}\")\n\n                except KeyboardInterrupt:\n                    cleanup()\n         \n    # Save checkpoint\n    if step % save_interval == 0:\n        save_model(step)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T14:46:22.917604Z","iopub.execute_input":"2025-04-03T14:46:22.917970Z","execution_failed":"2025-04-03T14:47:01.567Z"}},"outputs":[{"name":"stdout","text":"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0') tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0')\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0') tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0')\n","output_type":"stream"}],"execution_count":null}]}